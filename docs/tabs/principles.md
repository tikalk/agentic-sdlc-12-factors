# The Core Principles


## The Twelve Core Principles of Agentic Development.

In the modern era, software is commonly delivered as a service. Agentic SDLC represents a fundamental shift in how we build this software, where AI coding agents become integral participants throughout the software development lifecycle. This manifesto provides a set of principles for developers and teams to master this new paradigm, moving from ad-hoc prompting to structured, high-velocity collaboration.

**I. Strategic Mindset: Developer as Orchestrator, AI as Intern \
***Treat AI as a fast, knowledgeable junior partner that requires clear direction, mentorship, and rigorous review.*

**II. Context Scaffolding: Treat Context as a Dependency \
***Manage all context—code, documentation, and even the AI models themselves—with the same rigor as a critical software library.*

**III. Mission Definition: Start with a Formal Brief \
***Initiate every complex task with an explicit Mission Brief that defines goals, constraints, and success criteria before writing the first prompt.*

**IV. Structured Planning: Decompose and Triage Tasks \
***Use AI to generate a detailed execution plan, which the human developer reviews, approves, and triages into synchronous (interactive) and asynchronous (delegated) sub-tasks.*

**V. Dual Execution Loops: Pair Program or Delegate Toil \
***Master two distinct workflows: real-time synchronous collaboration for complex problems and asynchronous delegation to autonomous agents for well-defined tasks.*

**VI. The Great Filter: Apply Irreplaceable Human Judgment \
***The developer is the ultimate arbiter of quality, filtering all AI output for correctness, architectural cohesion, security, and domain-specific taste before it enters the codebase.*

**VII. Adaptive Quality Gates: Review Appropriately for Each Workflow \
***Implement continuous micro-reviews for synchronous work and formal, automated macro-reviews (e.g., Pull Requests, CI pipelines) for asynchronous work.*

**VIII. AI-Augmented, Risk-Based Testing \
***Use AI to make a risk-based testing strategy practical and scalable. The developer defines what risks matter; the AI generates the specific and targeted tests needed to validate them, moving beyond generic code coverage.*

**IX. Structured Traces: Document the 'Why,' Not Just the 'What' \
***Generate and preserve structured execution traces—the prompts, tool calls, and reasoning steps—as the primary artifact for debugging and process improvement.*

**X. Strategic Tooling: Manage a Federated, Governed Stack \
***Select and integrate a suite of specialized tools (IDEs, local agent runners, context repositories). routed through a central gateway to ensure control over cost, security, and model choice.*

**XI. Directives as Code: Version and Share AI Behavior \
***Treat your team's prompts, behavioral instructions, and system templates as a version-controlled asset, managed with the same discipline as application code.*

**XII. Team Capability: Systematize Learning and Improvement \
***Build organizational muscle memory by formalizing the sharing of best practices and using a versioned suite of evaluations (Evals) to objectively measure performance.*
